use crate::classifier::{self, ClassifierConfig, ImageClassifier};
use crate::detector::{DuplicateDetector, ImageAnalysis, ImageFeatures, ImageMetadata};
use crate::rename::ensure_guid_name;
use crate::resolution::{resolution_tier, ResolutionTier};
use crate::thumbnails::ThumbnailCache;
use indicatif::ProgressBar;
use rayon::prelude::*;
use rustc_hash::FxHashMap;
use serde::{Deserialize, Serialize};
use std::path::{Path, PathBuf};
use std::sync::{Arc, Mutex};
use walkdir::WalkDir;

const BUCKET_PREFIX_BITS: u32 = 48;

#[derive(Clone, Copy, Debug, PartialEq, Eq)]
pub enum ThreadingMode {
    Parallel,
    Sequential,
}

/// Parameters that control how the scanning pipeline behaves.
#[derive(Clone, Debug)]
pub struct ScanConfig {
    pub extensions: Vec<String>,
    pub threading: ThreadingMode,
    pub thumbnail_cache_root: Option<PathBuf>,
    /// When true, renames files to GUID format before processing.
    pub rename_to_guid: bool,
    /// When true, tags images below FHD resolution thresholds.
    pub detect_low_resolution: bool,
    /// When true, runs AI classification (moderation + tagging) on each image.
    pub enable_classification: bool,
}

impl ScanConfig {
    /// Builds a new configuration from the supplied extensions and threading mode.
    pub fn new(extensions: Vec<String>, threading: ThreadingMode) -> Self {
        Self {
            extensions,
            threading,
            thumbnail_cache_root: None,
            rename_to_guid: false,
            detect_low_resolution: false,
            enable_classification: false,
        }
    }

    pub fn with_thumbnail_root(mut self, root: PathBuf) -> Self {
        self.thumbnail_cache_root = Some(root);
        self
    }

    pub fn with_guid_rename(mut self, enabled: bool) -> Self {
        self.rename_to_guid = enabled;
        self
    }

    pub fn with_low_resolution_detection(mut self, enabled: bool) -> Self {
        self.detect_low_resolution = enabled;
        self
    }

    pub fn with_classification(mut self, enabled: bool) -> Self {
        self.enable_classification = enabled;
        self
    }
}

/// Maximum number of tags to keep per image.
pub const MAX_TAGS_PER_IMAGE: usize = 5;

/// A file entry that belongs to a duplicate group.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DuplicateEntry {
    pub path: PathBuf,
    pub size_bytes: u64,
    pub dimensions: (i32, i32),
    pub modified: Option<String>,
    pub captured_at: Option<String>,
    pub dominant_color: [u8; 3],
    pub confidence: f32,
    pub thumbnail: Option<PathBuf>,
    /// Resolution classification for the image.
    pub resolution_tier: ResolutionTier,
    /// AI moderation tier (if classification was enabled).
    #[serde(skip_serializing_if = "Option::is_none")]
    pub moderation_tier: Option<String>,
    /// AI-generated tags (if classification was enabled, max 5).
    #[serde(skip_serializing_if = "Vec::is_empty", default)]
    pub tags: Vec<String>,
}

/// A cluster of visually identical images identified during a scan.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct DuplicateGroup {
    pub fingerprint: u64,
    pub files: Vec<DuplicateEntry>,
}

/// Complete summary for a scan, suitable for serialisation.
#[derive(Debug, Clone, Serialize, Deserialize)]
pub struct ScanSummary {
    pub groups: Vec<DuplicateGroup>,
}

impl ScanSummary {
    /// Returns an iterator over groups that contain potential duplicates.
    pub fn duplicate_groups(&self) -> impl Iterator<Item = &DuplicateGroup> {
        self.groups.iter().filter(|group| group.files.len() > 1)
    }

    /// Returns an iterator over groups that need user attention:
    /// - Duplicate groups (2+ files with same fingerprint)
    /// - Singleton groups where the file has actionable resolution (Mobile or Low)
    pub fn actionable_groups(&self) -> impl Iterator<Item = &DuplicateGroup> {
        self.groups.iter().filter(|group| {
            group.files.len() > 1
                || group
                    .files
                    .first()
                    .map(|f| f.resolution_tier.is_actionable())
                    .unwrap_or(false)
        })
    }

    /// Indicates whether any groups were discovered.
    pub fn is_empty(&self) -> bool {
        self.groups.is_empty()
    }
}

pub fn count_entries(root: &Path) -> u64 {
    WalkDir::new(root).into_iter().count() as u64
}

pub fn scan(root: &Path, config: &ScanConfig, progress_bar: &Arc<ProgressBar>) -> ScanSummary {
    let detector = DuplicateDetector::default();
    let cache = ThumbnailCache::new(config.thumbnail_cache_root.clone())
        .ok()
        .map(Arc::new);

    // Initialize classifier if enabled
    let classifier: Option<Arc<Mutex<ImageClassifier>>> = if config.enable_classification {
        // Load classifier configuration
        let classifier_config = ClassifierConfig::load_or_default();
        
        // Initialize ONNX Runtime
        let ort_path = &classifier_config.ort_library;
        if let Err(e) = classifier::init_ort_runtime(ort_path) {
            progress_bar.set_message(format!("Classification disabled: {}", e));
            None
        } else {
            // Load classifier models from config
            match ImageClassifier::from_config(classifier_config) {
                Ok(c) => Some(Arc::new(Mutex::new(c))),
                Err(e) => {
                    progress_bar.set_message(format!("Classification disabled: {}", e));
                    None
                }
            }
        }
    } else {
        None
    };

    let groups = match config.threading {
        ThreadingMode::Parallel => {
            scan_parallel(root, config, progress_bar, &detector, &cache, &classifier)
        }
        ThreadingMode::Sequential => {
            scan_sequential(root, config, progress_bar, &detector, &cache, &classifier)
        }
    };

    ScanSummary { groups }
}

fn scan_parallel(
    root: &Path,
    config: &ScanConfig,
    progress_bar: &Arc<ProgressBar>,
    detector: &DuplicateDetector,
    cache: &Option<Arc<ThumbnailCache>>,
    classifier: &Option<Arc<Mutex<ImageClassifier>>>,
) -> Vec<DuplicateGroup> {
    let records = WalkDir::new(root)
        .into_iter()
        .par_bridge()
        .filter_map(|entry| {
            handle_entry(entry, config, progress_bar, detector, cache.as_deref(), classifier)
        })
        .fold(Vec::new, |mut collection, record| {
            collection.push(record);
            collection
        })
        .reduce(Vec::new, |mut left, mut right| {
            left.append(&mut right);
            left
        });

    group_records(records, detector)
}

fn scan_sequential(
    root: &Path,
    config: &ScanConfig,
    progress_bar: &Arc<ProgressBar>,
    detector: &DuplicateDetector,
    cache: &Option<Arc<ThumbnailCache>>,
    classifier: &Option<Arc<Mutex<ImageClassifier>>>,
) -> Vec<DuplicateGroup> {
    let mut records = Vec::new();
    for entry in WalkDir::new(root) {
        if let Some(record) =
            handle_entry(entry, config, progress_bar, detector, cache.as_deref(), classifier)
        {
            records.push(record);
        }
    }
    group_records(records, detector)
}

fn handle_entry(
    entry: Result<walkdir::DirEntry, walkdir::Error>,
    config: &ScanConfig,
    progress_bar: &Arc<ProgressBar>,
    detector: &DuplicateDetector,
    cache: Option<&ThumbnailCache>,
    classifier: &Option<Arc<Mutex<ImageClassifier>>>,
) -> Option<ImageRecord> {
    progress_bar.inc(1);
    match entry {
        Ok(entry) => {
            let mut path = entry.path().to_path_buf();
            progress_bar.set_message(format!("Scanning: {}", path.display()));
            if path.is_file() && has_image_extension(&path, &config.extensions) {
                if config.rename_to_guid {
                    match ensure_guid_name(&path) {
                        Ok(new_path) => {
                            if new_path != path {
                                progress_bar.set_message(format!(
                                    "Renamed: {} -> {}",
                                    path.file_name().unwrap_or_default().to_string_lossy(),
                                    new_path.file_name().unwrap_or_default().to_string_lossy()
                                ));
                            }
                            path = new_path;
                        }
                        Err(error) => {
                            progress_bar.set_message(format!("Rename error: {}", error));
                            return None;
                        }
                    }
                }

                match detector.analyze(&path) {
                    Ok(mut analysis) => {
                        if let Some(cache) = cache {
                            match cache.ensure(&path, analysis.features.fingerprint) {
                                Ok(thumbnail) => {
                                    analysis.metadata.thumbnail = Some(thumbnail);
                                }
                                Err(error) => {
                                    progress_bar.set_message(format!("Thumbnail error: {}", error));
                                }
                            }
                        }

                        if config.detect_low_resolution {
                            let (w, h) = analysis.metadata.dimensions;
                            analysis.metadata.resolution_tier = resolution_tier(w, h);
                        }

                        // Run AI classification if enabled
                        if let Some(classifier) = classifier {
                            if let Ok(mut clf) = classifier.lock() {
                                // Run moderation
                                match clf.moderate(&path) {
                                    Ok(flags) => {
                                        analysis.metadata.moderation_tier =
                                            Some(flags.tier.to_string());
                                    }
                                    Err(e) => {
                                        progress_bar.set_message(format!(
                                            "Moderation error: {}",
                                            e
                                        ));
                                    }
                                }

                                // Run tagging (max 5 tags)
                                match clf.tag(&path, MAX_TAGS_PER_IMAGE) {
                                    Ok(tags) => {
                                        analysis.metadata.tags =
                                            tags.into_iter().map(|t| t.label).collect();
                                    }
                                    Err(e) => {
                                        progress_bar
                                            .set_message(format!("Tagging error: {}", e));
                                    }
                                }
                            }
                        }

                        return Some(ImageRecord { path, analysis });
                    }
                    Err(error) => {
                        progress_bar.set_message(format!("Error: {}", error));
                    }
                }
            }
        }
        Err(error) => {
            progress_bar.set_message(format!("Error: {}", error));
        }
    }
    None
}

fn has_image_extension(path: &Path, extensions: &[String]) -> bool {
    path.extension()
        .and_then(|ext| ext.to_str())
        .map(|ext| {
            let lower = ext.to_lowercase();
            extensions.iter().any(|candidate| candidate == &lower)
        })
        .unwrap_or(false)
}

fn group_records(records: Vec<ImageRecord>, detector: &DuplicateDetector) -> Vec<DuplicateGroup> {
    let mut groups = Vec::new();
    let mut buckets: FxHashMap<u16, Vec<usize>> = FxHashMap::default();
    for record in records {
        insert_record(&mut groups, &mut buckets, record, detector);
    }

    groups
        .into_iter()
        .map(|state| DuplicateGroup {
            fingerprint: state.features.fingerprint,
            files: state
                .entries
                .into_iter()
                .map(|entry| {
                    let AnalyzedFile { path, metadata } = entry;
                    DuplicateEntry {
                        path,
                        size_bytes: metadata.size_bytes,
                        dimensions: metadata.dimensions,
                        modified: metadata.modified,
                        captured_at: metadata.captured_at,
                        dominant_color: metadata.dominant_color,
                        confidence: metadata.confidence,
                        thumbnail: metadata.thumbnail,
                        resolution_tier: metadata.resolution_tier,
                        moderation_tier: metadata.moderation_tier,
                        tags: metadata.tags,
                    }
                })
                .collect(),
        })
        .collect()
}

fn insert_record(
    groups: &mut Vec<GroupState>,
    buckets: &mut FxHashMap<u16, Vec<usize>>,
    record: ImageRecord,
    detector: &DuplicateDetector,
) {
    let ImageRecord { path, analysis } = record;
    let ImageAnalysis { features, metadata } = analysis;
    let bucket = bucket_id(features.fingerprint);
    if let Some(indices) = buckets.get(&bucket) {
        for &index in indices {
            if detector.is_similar(&groups[index].features, &features) {
                groups[index].entries.push(AnalyzedFile {
                    path: path.clone(),
                    metadata: metadata.clone(),
                });
                return;
            }
        }
    }

    for (index, group) in groups.iter_mut().enumerate() {
        if detector.is_similar(&group.features, &features) {
            group.entries.push(AnalyzedFile {
                path: path.clone(),
                metadata: metadata.clone(),
            });
            ensure_bucket_mapping(buckets, bucket, index);
            return;
        }
    }

    let index = groups.len();
    groups.push(GroupState {
        features,
        entries: vec![AnalyzedFile { path, metadata }],
    });
    ensure_bucket_mapping(buckets, bucket, index);
}

fn ensure_bucket_mapping(buckets: &mut FxHashMap<u16, Vec<usize>>, bucket: u16, index: usize) {
    let entry = buckets.entry(bucket).or_default();
    if !entry.contains(&index) {
        entry.push(index);
    }
}

fn bucket_id(fingerprint: u64) -> u16 {
    (fingerprint >> BUCKET_PREFIX_BITS) as u16
}

struct ImageRecord {
    path: PathBuf,
    analysis: ImageAnalysis,
}

struct GroupState {
    features: ImageFeatures,
    entries: Vec<AnalyzedFile>,
}

struct AnalyzedFile {
    path: PathBuf,
    metadata: ImageMetadata,
}

#[cfg(test)]
mod tests {
    use super::*;
    use indicatif::ProgressBar;
    use opencv::core::{self, Scalar, Vector};
    use opencv::imgcodecs;
    use std::sync::Arc;
    use tempfile::tempdir;

    fn write_image(path: &Path, color: u8) {
        let image = core::Mat::new_rows_cols_with_default(
            64,
            64,
            core::CV_8UC3,
            Scalar::from((color as f64, color as f64, color as f64, 0.0)),
        )
        .unwrap();
        let params = Vector::<i32>::new();
        imgcodecs::imwrite(path.to_string_lossy().as_ref(), &image, &params).unwrap();
    }

    fn scan_duplicates(mode: ThreadingMode) {
        let dir = tempdir().unwrap();
        let thumb_dir = tempdir().unwrap();
        let first = dir.path().join("a.jpg");
        let second = dir.path().join("b.jpg");
        let third = dir.path().join("c.png");
        write_image(&first, 64);
        write_image(&second, 64);
        write_image(&third, 200);
        let progress = Arc::new(ProgressBar::hidden());
        let config = ScanConfig::new(vec![String::from("jpg"), String::from("png")], mode)
            .with_thumbnail_root(thumb_dir.path().to_path_buf());
        let summary = scan(dir.path(), &config, &progress);
        let duplicates: Vec<_> = summary.duplicate_groups().collect();
        assert_eq!(duplicates.len(), 1);
        let files = &duplicates[0].files;
        let paths: Vec<_> = files.iter().map(|entry| entry.path.clone()).collect();
        assert!(paths.contains(&first));
        assert!(paths.contains(&second));
        assert!(files.iter().all(|entry| entry.size_bytes > 0));
        assert!(files.iter().all(|entry| entry.dimensions == (64, 64)));
        assert!(files.iter().all(|entry| entry
            .thumbnail
            .as_ref()
            .map(|path| path.exists())
            .unwrap_or(false)));
        assert!(summary
            .groups
            .iter()
            .any(|group| group.files.len() == 1 && group.files[0].path == third));
    }

    #[test]
    fn scan_detects_duplicates_parallel() {
        scan_duplicates(ThreadingMode::Parallel);
    }

    #[test]
    fn scan_detects_duplicates_sequential() {
        scan_duplicates(ThreadingMode::Sequential);
    }

    #[test]
    fn count_entries_includes_root_and_files() {
        let dir = tempdir().unwrap();
        write_image(&dir.path().join("a.jpg"), 0);
        write_image(&dir.path().join("b.jpg"), 128);
        write_image(&dir.path().join("c.jpg"), 255);
        assert_eq!(count_entries(dir.path()), 4);
    }
}
